{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09964295-6e34-409f-879c-01c0a17d692d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ndlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "270c6da5-812a-4d0d-8122-16c69c8fd876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "import glob\n",
    "from ndlinear import NdLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b754cc-2495-4157-8a38-deddd7433987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data_dir, captions_file, tokenizer, transform, max_length=64):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.data = self._load_captions(captions_file)\n",
    "        print(f\"Loaded {len(self.data)} image-caption pairs\")\n",
    "\n",
    "    def _load_captions(self, filepath):\n",
    "        data = []\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Split only on first comma to separate image name and caption\n",
    "                parts = line.split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    img_name, caption = parts[0].strip(), parts[1].strip()\n",
    "                    \n",
    "                    # Remove quotes if present\n",
    "                    if caption.startswith('\"') and caption.endswith('\"'):\n",
    "                        caption = caption[1:-1]\n",
    "                    \n",
    "                    data.append((img_name, caption))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.data[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        \n",
    "        # Handle image loading errors gracefully\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a placeholder image (black)\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Encode the caption\n",
    "        encoded = self.tokenizer(\n",
    "            caption, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"caption\": caption,  # Store original caption for evaluation\n",
    "            \"image_name\": img_name  # Store image name for evaluation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd51a104-4e4f-490d-9136-7661ab15fd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim):\n",
    "        super().__init__()\n",
    "        # self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # self.projection = nn.Linear(embed_dim, proj_dim)\n",
    "        # self.layer_norm = nn.LayerNorm(proj_dim)\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.projection = NdLinear(input_dims=(embed_dim,), hidden_size=(proj_dim,))\n",
    "        self.layer_norm = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x = x[:, 0, :]  # B, T[cls], E\n",
    "        x = self.projection(x)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0939eb24-f9cd-453d-bfa2-dc4334b7e536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224', proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.model = ViTModel.from_pretrained(model_name)\n",
    "        embed_dim = self.model.config.hidden_size\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # self.projection = nn.Linear(embed_dim, proj_dim)\n",
    "        # self.layer_norm = nn.LayerNorm(proj_dim)\n",
    "        self.projection = NdLinear(input_dims=(embed_dim,), hidden_size=(proj_dim,))\n",
    "        self.layer_norm = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.model(pixel_values=x)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        x = self.projection(cls_token)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c21dd9ef-9d54-4c47-814b-cd3ce050a5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224', proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(model_name, proj_dim)\n",
    "        self.text_encoder = TextEncoder(embed_dim=768, proj_dim=proj_dim)\n",
    "        # Initialize temperature parameter (learnable)\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n",
    "\n",
    "    def forward(self, batch, device):\n",
    "        # Move inputs to device\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        image_features = self.image_encoder(images)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_features = F.normalize(image_features, dim=1)\n",
    "        text_features = F.normalize(text_features, dim=1)\n",
    "\n",
    "        # Scaled pairwise cosine similarities [n, n]\n",
    "        logits = torch.matmul(image_features, text_features.T) * torch.exp(self.temperature)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        labels = torch.arange(image_features.size(0)).to(device)\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "        return loss, logits, image_features, text_features\n",
    "\n",
    "    def encode_image(self, image, device):\n",
    "        image = image.to(device)\n",
    "        features = self.image_encoder(image)\n",
    "        return F.normalize(features, dim=1)\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask, device):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        features = self.text_encoder(input_ids, attention_mask)\n",
    "        return F.normalize(features, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a076eb7b-c9fd-4f52-a1f2-de9b0b8cdb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in progress_bar:\n",
    "        loss, _, _, _ = model(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} completed in {elapsed:.2f}s - Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d21c174-0fca-47c1-86b1-4ba835205336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    i2t_correct_1 = 0\n",
    "    i2t_correct_5 = 0\n",
    "    t2i_correct_1 = 0\n",
    "    t2i_correct_5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    all_captions = []\n",
    "    all_image_names = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            loss, logits, image_features, text_features = model(batch, device)\n",
    "            \n",
    "            # Store features and metadata for analysis\n",
    "            all_image_features.append(image_features)\n",
    "            all_text_features.append(text_features)\n",
    "            all_captions.extend(batch[\"caption\"])\n",
    "            all_image_names.extend(batch[\"image_name\"])\n",
    "            \n",
    "            # Image-to-text retrieval (find the right caption for each image)\n",
    "            i2t_similarity = logits\n",
    "            i2t_sorted_indices = i2t_similarity.argsort(dim=1, descending=True)\n",
    "            \n",
    "            # Text-to-image retrieval (find the right image for each caption)\n",
    "            t2i_similarity = logits.T\n",
    "            t2i_sorted_indices = t2i_similarity.argsort(dim=1, descending=True)\n",
    "            \n",
    "            # Calculate batch accuracy\n",
    "            batch_size = image_features.size(0)\n",
    "            targets = torch.arange(batch_size).to(device)\n",
    "            \n",
    "            # Top-1 and Top-5 accuracy for image-to-text\n",
    "            i2t_correct_1 += (i2t_sorted_indices[:, 0] == targets).sum().item()\n",
    "            for k in range(min(5, batch_size)):\n",
    "                i2t_correct_5 += (i2t_sorted_indices[:, k] == targets).sum().item()\n",
    "            \n",
    "            # Top-1 and Top-5 accuracy for text-to-image\n",
    "            t2i_correct_1 += (t2i_sorted_indices[:, 0] == targets).sum().item()\n",
    "            for k in range(min(5, batch_size)):\n",
    "                t2i_correct_5 += (t2i_sorted_indices[:, k] == targets).sum().item()\n",
    "            \n",
    "            total += batch_size\n",
    "    \n",
    "    # Combine features for full dataset analysis\n",
    "    all_image_features = torch.cat(all_image_features, dim=0)\n",
    "    all_text_features = torch.cat(all_text_features, dim=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    i2t_top1_acc = i2t_correct_1 / total\n",
    "    i2t_top5_acc = i2t_correct_5 / (total * min(5, total))\n",
    "    t2i_top1_acc = t2i_correct_1 / total\n",
    "    t2i_top5_acc = t2i_correct_5 / (total * min(5, total))\n",
    "    \n",
    "    results = {\n",
    "        \"i2t_top1\": i2t_top1_acc,\n",
    "        \"i2t_top5\": i2t_top5_acc,\n",
    "        \"t2i_top1\": t2i_top1_acc,\n",
    "        \"t2i_top5\": t2i_top5_acc,\n",
    "        \"image_features\": all_image_features.cpu(),\n",
    "        \"text_features\": all_text_features.cpu(),\n",
    "        \"captions\": all_captions,\n",
    "        \"image_names\": all_image_names\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcb03844-e9d7-41a0-abbf-414eff1e6d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_shot_prediction(model, image_path, text_candidates, tokenizer, transform, device):\n",
    "    \"\"\"\n",
    "    Perform zero-shot prediction using the trained CLIP model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CLIP model\n",
    "        image_path: Path to the query image\n",
    "        text_candidates: List of textual descriptions to match against\n",
    "        tokenizer: Text tokenizer\n",
    "        transform: Image transformation pipeline\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        List of (text, similarity score) pairs sorted by score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Process text candidates\n",
    "    text_tokens = tokenizer(\n",
    "        text_candidates,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode image\n",
    "        image_features = model.encode_image(image, device)\n",
    "        \n",
    "        # Encode text candidates\n",
    "        text_features = model.encode_text(\n",
    "            text_tokens[\"input_ids\"], \n",
    "            text_tokens[\"attention_mask\"],\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = (image_features @ text_features.T).squeeze(0)\n",
    "        \n",
    "    # Sort by similarity\n",
    "    similarities = similarities.cpu().numpy()\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "    \n",
    "    # Return sorted text-similarity pairs\n",
    "    results = [(text_candidates[i], similarities[i]) for i in sorted_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f764ff-78e1-413a-84ce-fd68fe045756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, device, save_dir, epochs=20, save_interval=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate the CLIP model\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    val_metrics = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        avg_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_results = evaluate(model, val_loader, device)\n",
    "        val_metrics.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"i2t_top1\": val_results[\"i2t_top1\"],\n",
    "            \"i2t_top5\": val_results[\"i2t_top5\"],\n",
    "            \"t2i_top1\": val_results[\"t2i_top1\"],\n",
    "            \"t2i_top5\": val_results[\"t2i_top5\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"Validation metrics:\")\n",
    "        print(f\"  Image-to-Text: Top-1: {val_results['i2t_top1']:.4f}, Top-5: {val_results['i2t_top5']:.4f}\")\n",
    "        print(f\"  Text-to-Image: Top-1: {val_results['t2i_top1']:.4f}, Top-5: {val_results['t2i_top5']:.4f}\")\n",
    "        \n",
    "        # Save checkpoint at intervals\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f\"clip_model_epoch_{epoch+1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'val_metrics': val_metrics[-1]\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = os.path.join(save_dir, \"clip_model_final.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'val_metrics': val_metrics[-1]\n",
    "    }, final_path)\n",
    "    print(f\"Final model saved to {final_path}\")\n",
    "    \n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ffb2b9-e68c-4600-8bca-9aaaf8c7b7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_results(train_results, save_dir):\n",
    "    \"\"\"\n",
    "    Visualize training progress and metrics\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_results[\"train_losses\"])\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_dir, 'training_loss.png'))\n",
    "    \n",
    "    # Plot validation metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    epochs = [m[\"epoch\"] for m in train_results[\"val_metrics\"]]\n",
    "    plt.plot(epochs, [m[\"i2t_top1\"] for m in train_results[\"val_metrics\"]], label='I2T Top-1')\n",
    "    plt.plot(epochs, [m[\"i2t_top5\"] for m in train_results[\"val_metrics\"]], label='I2T Top-5')\n",
    "    plt.plot(epochs, [m[\"t2i_top1\"] for m in train_results[\"val_metrics\"]], label='T2I Top-1')\n",
    "    plt.plot(epochs, [m[\"t2i_top5\"] for m in train_results[\"val_metrics\"]], label='T2I Top-5')\n",
    "    plt.title('Validation Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_dir, 'validation_metrics.png'))\n",
    "    \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d15def31-0c3d-4508-a33f-99813c38f268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Configuration\n",
    "    data_dir = \"/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images\"  # Update with your path\n",
    "    captions_file = \"/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/captions.txt\"  # Update with your path\n",
    "    save_dir = \"./model_checkpoints\"\n",
    "    results_dir = \"./results\"\n",
    "    \n",
    "    # Model and training parameters\n",
    "    proj_dim = 256\n",
    "    batch_size = 32\n",
    "    num_epochs = 15\n",
    "    save_interval = 5\n",
    "    random_seed = 42\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize tokenizer and transforms\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = ImageTextDataset(\n",
    "        data_dir=data_dir,\n",
    "        captions_file=captions_file,\n",
    "        tokenizer=tokenizer,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split dataset into train and validation sets\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(dataset))), \n",
    "        test_size=0.1,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing CLIP model...\")\n",
    "    model = CLIPModel(\n",
    "        model_name='google/vit-base-patch16-224',\n",
    "        proj_dim=proj_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    print(\"Starting training...\")\n",
    "    train_results = train_and_evaluate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        epochs=num_epochs,\n",
    "        save_interval=save_interval\n",
    "    )\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"Visualizing results...\")\n",
    "    visualize_results(train_results, results_dir)\n",
    "    \n",
    "    # Perform zero-shot prediction example\n",
    "    print(\"\\nPerforming zero-shot prediction example:\")\n",
    "    # Use the first image from validation set as example\n",
    "    example_batch = next(iter(val_loader))\n",
    "    example_img_name = example_batch[\"image_name\"][0]\n",
    "    example_img_path = os.path.join(data_dir, example_img_name)\n",
    "    \n",
    "    # Create some sample text candidates (including the actual caption)\n",
    "    actual_caption = example_batch[\"caption\"][0]\n",
    "    text_candidates = [\n",
    "        actual_caption,\n",
    "        \"A dog running on the beach\",\n",
    "        \"A cat sitting on a window sill\",\n",
    "        \"A person hiking in the mountains\",\n",
    "        \"Children playing in a park\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Query image: {example_img_name}\")\n",
    "    print(f\"Actual caption: {actual_caption}\")\n",
    "    \n",
    "    results = zero_shot_prediction(\n",
    "        model=model,\n",
    "        image_path=example_img_path,\n",
    "        text_candidates=text_candidates,\n",
    "        tokenizer=tokenizer,\n",
    "        transform=transform,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"Zero-shot predictions (sorted by similarity):\")\n",
    "    for text, score in results:\n",
    "        print(f\"  Score: {score:.4f} - Text: {text}\")\n",
    "    \n",
    "    print(\"\\nTraining and evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d4c03cf-3212-450c-9321-4f58f1798aa8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Loaded 40456 image-caption pairs\n",
      "Training set size: 36410\n",
      "Validation set size: 4046\n",
      "Initializing CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 306/1138 [02:01<05:55,  2.34it/s, loss=0.639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1138/1138 [07:41<00:00,  2.46it/s, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 461.94s - Avg Loss: 0.6242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.8997, Top-5: 0.1991\n",
      "  Text-to-Image: Top-1: 0.8982, Top-5: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  70%|███████   | 802/1138 [05:09<02:06,  2.65it/s, loss=0.247] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1138/1138 [07:20<00:00,  2.59it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed in 440.02s - Avg Loss: 0.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9100, Top-5: 0.1991\n",
      "  Text-to-Image: Top-1: 0.9021, Top-5: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  82%|████████▏ | 931/1138 [06:00<01:23,  2.49it/s, loss=0.0457]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1138/1138 [07:20<00:00,  2.58it/s, loss=0.205] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed in 440.33s - Avg Loss: 0.1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9142, Top-5: 0.1992\n",
      "  Text-to-Image: Top-1: 0.9071, Top-5: 0.1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  39%|███▊      | 439/1138 [02:50<04:24,  2.64it/s, loss=0.0919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1138/1138 [07:20<00:00,  2.58it/s, loss=0.198] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed in 440.42s - Avg Loss: 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9135, Top-5: 0.1990\n",
      "  Text-to-Image: Top-1: 0.9093, Top-5: 0.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   8%|▊         | 92/1138 [00:35<06:44,  2.59it/s, loss=0.182] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   8%|▊         | 93/1138 [00:36<06:39,  2.62it/s, loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1138/1138 [07:20<00:00,  2.58it/s, loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed in 440.40s - Avg Loss: 0.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9152, Top-5: 0.1993\n",
      "  Text-to-Image: Top-1: 0.9105, Top-5: 0.1991\n",
      "Model checkpoint saved to ./model_checkpoints/clip_model_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  88%|████████▊ | 1002/1138 [06:28<00:55,  2.46it/s, loss=0.187] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1138/1138 [07:20<00:00,  2.58it/s, loss=0.0977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed in 440.82s - Avg Loss: 0.1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9187, Top-5: 0.1989\n",
      "  Text-to-Image: Top-1: 0.9197, Top-5: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   7%|▋         | 77/1138 [00:29<06:44,  2.62it/s, loss=0.0937] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1138/1138 [07:21<00:00,  2.57it/s, loss=0.233]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed in 441.98s - Avg Loss: 0.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9086, Top-5: 0.1990\n",
      "  Text-to-Image: Top-1: 0.9152, Top-5: 0.1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1138/1138 [07:21<00:00,  2.58it/s, loss=0.043] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed in 441.73s - Avg Loss: 0.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9187, Top-5: 0.1990\n",
      "  Text-to-Image: Top-1: 0.9194, Top-5: 0.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  19%|█▊        | 211/1138 [01:21<05:52,  2.63it/s, loss=0.0695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1138/1138 [07:21<00:00,  2.58it/s, loss=0.0709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed in 441.81s - Avg Loss: 0.0974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9147, Top-5: 0.1992\n",
      "  Text-to-Image: Top-1: 0.9182, Top-5: 0.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:  79%|███████▊  | 895/1138 [05:47<01:33,  2.60it/s, loss=0.115]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1138/1138 [07:21<00:00,  2.57it/s, loss=0.0242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed in 441.97s - Avg Loss: 0.0974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9207, Top-5: 0.1994\n",
      "  Text-to-Image: Top-1: 0.9207, Top-5: 0.1992\n",
      "Model checkpoint saved to ./model_checkpoints/clip_model_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:  31%|███       | 352/1138 [02:16<05:01,  2.60it/s, loss=0.0462] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1138/1138 [07:21<00:00,  2.58it/s, loss=0.0844] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed in 441.47s - Avg Loss: 0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9219, Top-5: 0.1994\n",
      "  Text-to-Image: Top-1: 0.9197, Top-5: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:  32%|███▏      | 361/1138 [02:19<04:57,  2.61it/s, loss=0.189]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1138/1138 [07:20<00:00,  2.58it/s, loss=0.026]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed in 440.70s - Avg Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:36<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9145, Top-5: 0.1989\n",
      "  Text-to-Image: Top-1: 0.9152, Top-5: 0.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13:  14%|█▍        | 160/1138 [01:01<06:09,  2.65it/s, loss=0.102]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1138/1138 [07:21<00:00,  2.58it/s, loss=0.0588] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 completed in 441.87s - Avg Loss: 0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:37<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9184, Top-5: 0.1992\n",
      "  Text-to-Image: Top-1: 0.9286, Top-5: 0.1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14:  36%|███▌      | 409/1138 [02:38<04:56,  2.46it/s, loss=0.0549] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1138/1138 [07:22<00:00,  2.57it/s, loss=0.103]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 completed in 442.19s - Avg Loss: 0.0799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:38<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9204, Top-5: 0.1990\n",
      "  Text-to-Image: Top-1: 0.9157, Top-5: 0.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  84%|████████▎ | 951/1138 [06:09<01:11,  2.60it/s, loss=0.0536] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image: [Errno 2] No such file or directory: '/home/b.gandhi/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1138/1138 [07:21<00:00,  2.58it/s, loss=0.02]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed in 441.51s - Avg Loss: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 127/127 [00:37<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "  Image-to-Text: Top-1: 0.9236, Top-5: 0.1990\n",
      "  Text-to-Image: Top-1: 0.9256, Top-5: 0.1989\n",
      "Model checkpoint saved to ./model_checkpoints/clip_model_epoch_15.pt\n",
      "Final model saved to ./model_checkpoints/clip_model_final.pt\n",
      "Visualizing results...\n",
      "\n",
      "Performing zero-shot prediction example:\n",
      "Query image: 3606093421_eddd46c2c7.jpg\n",
      "Actual caption: two men in an orange raft boat\n",
      "Zero-shot predictions (sorted by similarity):\n",
      "  Score: 0.6272 - Text: two men in an orange raft boat\n",
      "  Score: 0.0257 - Text: A dog running on the beach\n",
      "  Score: 0.0018 - Text: A person hiking in the mountains\n",
      "  Score: -0.0535 - Text: A cat sitting on a window sill\n",
      "  Score: -0.3343 - Text: Children playing in a park\n",
      "\n",
      "Training and evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5281b9ae-f549-4d3a-b3e6-26a030f6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def test_random_image(checkpoint_path, image_dir):\n",
    "    \"\"\"\n",
    "    Test the CLIP model on a random image from the specified directory\n",
    "    \"\"\"\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model and load checkpoint\n",
    "    model = CLIPModel(proj_dim=256).to(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    \n",
    "    # Initialize tokenizer and image transform\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    # Define some test classes for zero-shot classification\n",
    "    classes = [\n",
    "        \"dog\", \"cat\", \"bird\", \"person\", \"car\", \"mountain\", \"beach\", \n",
    "        \"building\", \"flower\", \"tree\", \"food\", \"sunset\", \"garden\", \"trees\", \"city\"\n",
    "    ]\n",
    "\n",
    "    # Define some test captions\n",
    "    captions = [\n",
    "        \"A dog running in a field\",\n",
    "        \"A cat sleeping on a couch\",\n",
    "        \"A person hiking in the mountains\",\n",
    "        \"A beautiful sunset over the ocean\",\n",
    "        \"Children playing in a park\",\n",
    "        \"A car driving on a highway\",\n",
    "        \"A building in a city skyline\",\n",
    "        \"Flowers in a garden\",\n",
    "        \"A plate of delicious food\",\n",
    "        \"A dog sitting on the beach\",\n",
    "        \"A view of garden and skyscrapers\"\n",
    "    ]\n",
    "    image_paths = glob.glob(image_dir + \"*.jpeg\")\n",
    "    for img_path in image_paths:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Test zero-shot classification\n",
    "        print(\"\\nPerforming zero-shot classification...\")\n",
    "        with torch.no_grad():\n",
    "            # Encode image\n",
    "            image_features = model.encode_image(image_tensor, device)\n",
    "\n",
    "            # Create text prompts using a template\n",
    "            text_prompts = [f\"a photo of a {cls}\" for cls in classes]\n",
    "\n",
    "            # Tokenize text\n",
    "            text_tokens = tokenizer(\n",
    "                text_prompts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=64,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Encode text\n",
    "            text_features = model.encode_text(\n",
    "                text_tokens[\"input_ids\"], \n",
    "                text_tokens[\"attention_mask\"],\n",
    "                device\n",
    "            )\n",
    "\n",
    "            # Calculate similarities\n",
    "            similarities = (image_features @ text_features.T).squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Sort by similarity\n",
    "            sorted_indices = similarities.argsort()[::-1]\n",
    "\n",
    "            # Print results\n",
    "            print(\"Top 5 classifications:\")\n",
    "            for i in sorted_indices[:5]:\n",
    "                print(f\"  {classes[i]}: {similarities[i]:.4f}\")\n",
    "\n",
    "        # Test caption matching\n",
    "        print(\"\\nPerforming caption matching...\")\n",
    "        with torch.no_grad():\n",
    "            # Tokenize captions\n",
    "            caption_tokens = tokenizer(\n",
    "                captions,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=64,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Encode captions\n",
    "            caption_features = model.encode_text(\n",
    "                caption_tokens[\"input_ids\"], \n",
    "                caption_tokens[\"attention_mask\"],\n",
    "                device\n",
    "            )\n",
    "\n",
    "            # Calculate similarities\n",
    "            similarities = (image_features @ caption_features.T).squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Sort by similarity\n",
    "            sorted_indices = similarities.argsort()[::-1]\n",
    "\n",
    "            # Print results\n",
    "            print(\"Top 3 matching captions:\")\n",
    "            for i in sorted_indices[:3]:\n",
    "                print(f\"  Score: {similarities[i]:.4f} - {captions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f22330-218c-424f-a08f-2640993e2ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./model_checkpoints/clip_model_final.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1e28f16-38de-43f3-b8aa-40cf09235896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_dir = \"../CLIP/test_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2553388c-9db1-4c18-ae3e-e00ed2e75af3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from ./model_checkpoints/clip_model_final.pt\n",
      "\n",
      "Performing zero-shot classification...\n",
      "Top 5 classifications:\n",
      "  beach: 0.4875\n",
      "  sunset: 0.4549\n",
      "  dog: 0.3377\n",
      "  food: 0.3081\n",
      "  flower: 0.2660\n",
      "\n",
      "Performing caption matching...\n",
      "Top 3 matching captions:\n",
      "  Score: 0.5803 - A dog sitting on the beach\n",
      "  Score: 0.5247 - A beautiful sunset over the ocean\n",
      "  Score: 0.2511 - A dog running in a field\n",
      "\n",
      "Performing zero-shot classification...\n",
      "Top 5 classifications:\n",
      "  city: 0.5318\n",
      "  garden: 0.4198\n",
      "  bird: 0.3731\n",
      "  flower: 0.3682\n",
      "  tree: 0.3492\n",
      "\n",
      "Performing caption matching...\n",
      "Top 3 matching captions:\n",
      "  Score: 0.7126 - A view of garden and skyscrapers\n",
      "  Score: 0.6368 - A building in a city skyline\n",
      "  Score: 0.3620 - Flowers in a garden\n"
     ]
    }
   ],
   "source": [
    "test_random_image(checkpoint_path, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17901c08-8d9b-4593-b6d3-58903b37f074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
